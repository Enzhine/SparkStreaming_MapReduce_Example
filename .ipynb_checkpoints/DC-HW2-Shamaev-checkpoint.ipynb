{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51a92df6ec7b746f",
   "metadata": {},
   "source": [
    "# Распределенные вычисления ДЗ-2 | Шамаев Онар Евгеньевич "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7db5f2c875c41a",
   "metadata": {},
   "source": [
    "## Постановка задачи\n",
    "\n",
    "Необходимо в потоковом формате считывать последнюю активность сообщества r/AskReddit платформы Reddit.\n",
    "Целью будет найти самые встречающиеся слова.\n",
    "Необходимо использовать Spark Streaming сохраняя данные на HDFS.\n",
    "\n",
    "## План\n",
    "1. Создание TCP сервера (RSS читателя заголовков)\n",
    "2. Запуск HDFS\n",
    "3. Написать MapReduce для Spark Streaming\n",
    "4. Результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed36cf5044f39634",
   "metadata": {},
   "source": [
    "## 1. Создание TCP сервера (RSS читателя заголовков)\n",
    "\n",
    "Идея технологии RSS заключается в том, чтобы предоставлять последние N текстовых информативных блоков чего-либо. Например на новостных сайтах - это краткая сводка о последних события. В случае Reddit - это список последних назвний последних тем, поднимаемых пользователями в сообществе r/AskReddit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59edecd3c69378db",
   "metadata": {},
   "source": [
    "_Импорты библиотек._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c187257be5d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import socket\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44f67fa79f7eb7",
   "metadata": {},
   "source": [
    "Последние новости (в RSS формате) доступны по ссылке:\n",
    "`https://www.reddit.com/r/AskReddit/new/.rss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3c96c2e9463f890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:35:29.320248Z",
     "start_time": "2025-03-07T23:35:29.316579Z"
    }
   },
   "outputs": [],
   "source": [
    "rss_link = 'https://www.reddit.com/r/AskReddit/new/.rss'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb3a1a7b5567a7",
   "metadata": {},
   "source": [
    "Определим программу читателя новостной ленты RSS и извлекающей оттуда заголовки, создающий TCP сокет и отправляющий их туда."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:35:31.290957Z",
     "start_time": "2025-03-07T23:35:31.284047Z"
    }
   },
   "outputs": [],
   "source": [
    "encoding = 'utf-8'\n",
    "\n",
    "\n",
    "def rss2tcp_reader(ip, port, interval_sec: float = 30.0, ttl=None):\n",
    "    last_stamp: time.struct_time | None = None\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.bind((ip, port))\n",
    "    s.listen(1)\n",
    "\n",
    "    print(f'Awaiting connection at {ip}:{port}...')\n",
    "    conn, addr = s.accept()\n",
    "    print('Connected by ', addr)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            if (ttl is not None) and last_stamp > ttl:\n",
    "                print('Time to live passed. Exiting.')\n",
    "                break\n",
    "\n",
    "            feed = feedparser.parse(rss_link)\n",
    "            if feed.status != 200:\n",
    "                print(f'Bad response {feed.status} received! Exiting.')\n",
    "                break\n",
    "\n",
    "            _mx_stmp = None\n",
    "            for i, entry in enumerate(feed.entries):\n",
    "                title = entry.title\n",
    "                time_stamp = entry.published_parsed\n",
    "\n",
    "                if (last_stamp is None) or (last_stamp < time_stamp):\n",
    "                    if _mx_stmp is None: _mx_stmp = time_stamp\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "                conn.send(title.encode(encoding) + b'\\n')\n",
    "            if _mx_stmp is not None:\n",
    "                last_stamp = _mx_stmp\n",
    "\n",
    "            time.sleep(interval_sec)\n",
    "    except Exception as ex:\n",
    "        print(f'Exception happened {ex}')\n",
    "    finally:\n",
    "        s.close()\n",
    "        print(f'Closed connection')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977904fc9140cece",
   "metadata": {},
   "source": [
    "Данная программа представлена отдельно в файле `tcp_server.py`. Запустим ее независмо, чтобы можно было запускать ячейки ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd598312c1dbb54",
   "metadata": {},
   "source": [
    "## 2. Запуск HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1b92c7bdd23410",
   "metadata": {},
   "source": [
    "Воспользуемся наработками 1 дз. Поднимем HDFS кластер из 1 namenode и 2 datanode с помощью Docker.\n",
    "\n",
    "Запустим кластер из папки `compose` командой `docker-compose -f \"docker.compose.yml\" up -d`.\n",
    "\n",
    "UI кластера доступен по адресу `http://localhost:9870/`.\n",
    "\n",
    "Сама HDFS доступна по адресу `hdfs://localhost:8020`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c895aa2a86aec70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:35:37.881064Z",
     "start_time": "2025-03-07T23:35:37.878176Z"
    }
   },
   "outputs": [],
   "source": [
    "hdfs = 'hdfs://localhost:8020'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41eee99e9b48e03",
   "metadata": {},
   "source": [
    "## 3. Написать MapReduce для Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e242cd821b967efc",
   "metadata": {},
   "source": [
    "_Импорты библиотек._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a432bd927b9c64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:35:44.242264Z",
     "start_time": "2025-03-07T23:35:43.422350Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd59f7a4c2105a4",
   "metadata": {},
   "source": [
    "Создадим Спарк-контекст и Стриминговый контекст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "481b085bab7138fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:36:04.236625Z",
     "start_time": "2025-03-07T23:35:46.463067Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ctx = SparkContext(master=\"local[2]\", appName=\"AskRedditWordCounter\")\n",
    "spark = SparkSession(ctx)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11c642ae033e0d0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:36:06.833515Z",
     "start_time": "2025-03-07T23:36:06.785452Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enzhine\\miniconda3\\envs\\sparked\\Lib\\site-packages\\pyspark\\streaming\\context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handling store at 2025-03-14 03:17:40\n",
      "set top to ['you', 'what', 'the', 'is', 'and'] at 2025-03-14 03:17:40\n",
      "handling store at 2025-03-14 03:18:10\n",
      "set top to ['you', 'what', 'the', 'is', 'your'] at 2025-03-14 03:18:10\n",
      "handling store at 2025-03-14 03:18:40\n"
     ]
    }
   ],
   "source": [
    "ctx_stream = StreamingContext(ctx, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec2e864c9d2a548",
   "metadata": {},
   "source": [
    "PySpark UI доступен по адресу `http://localhost:4040`.\n",
    "\n",
    "Будем использовать DStream, считывающий данные по TCP соединению, созданном нашим RSS читателем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a146b0e741d29b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:36:15.117157Z",
     "start_time": "2025-03-07T23:36:15.074467Z"
    }
   },
   "outputs": [],
   "source": [
    "ip = 'localhost'\n",
    "port = 9999\n",
    "\n",
    "d_stream = ctx_stream.socketTextStream(ip, port)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82affc2efc435c8d",
   "metadata": {},
   "source": [
    "Определим фильтр, для подготовки к разбиению заголовков на слова, и непосредственно разделитель на слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64c5adb4312582ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:36:17.722114Z",
     "start_time": "2025-03-07T23:36:17.717951Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def prepare(line: str) -> str:\n",
    "    return line.translate({key: ' ' for key in string.punctuation + '?!()$@/'}).lower()\n",
    "\n",
    "\n",
    "def word_splitter(line: str) -> list[str]:\n",
    "    return filter(lambda w: len(w) != 0, line.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2080453a720bc6",
   "metadata": {},
   "source": [
    "Определим обработчик RDD по топ, получаемых DStream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8dedca107976b1",
   "metadata": {},
   "source": [
    "Определим порядок действий джобы Spark Stream:\n",
    "- разделяем входные заголовки на слова;\n",
    "- map в пару (слово, количество), где количество всегда равно 1;\n",
    "- обновление состояния по ключу (reduce) путем сложения количеств;\n",
    "- запись в кластер hdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "405b2884-422a-405b-bb2d-f3ad5c66a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_lines = d_stream.window(windowDuration=60, slideDuration=30)\n",
    "\n",
    "\n",
    "def store(time_, rdd):\n",
    "    print(f'handling store at {time_}')\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "\n",
    "    print('storing')\n",
    "    df = rdd.map(lambda line: (line,)).toDF(schema=[\"titles\"])\n",
    "    df.write.format('json').mode('append').save(hdfs + '/reddit_titles.json')\n",
    "\n",
    "\n",
    "current_top = []\n",
    "\n",
    "\n",
    "def filter_by_top(line):\n",
    "    global current_top\n",
    "\n",
    "    print(f'checked {line} for contain of {current_top}')\n",
    "    return any(word in line for word in current_top)\n",
    "\n",
    "windowed_lines_cached = windowed_lines.transform(lambda rdd: rdd.filter(filter_by_top)).foreachRDD(store)\n",
    "\n",
    "\n",
    "def handle_top(time_, rdd):\n",
    "    global current_top\n",
    "    current_top = rdd.map(lambda pair: pair[0]).take(5)\n",
    "    print(f'set top to {current_top} at {time_}')\n",
    "\n",
    "\n",
    "windowed_lines.flatMap(lambda line: word_splitter(prepare(line))) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .transform(lambda rdd: rdd.sortBy(lambda x: -x[1])) \\\n",
    "    .foreachRDD(handle_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be61264-51b9-4255-89c1-cca98b0caaa6",
   "metadata": {},
   "source": [
    "Запустим джобу на 10 минут. Теперь она будет работать на фоне и собирать статистику встречаемых слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "603969914b3e94e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:38:35.951154Z",
     "start_time": "2025-03-07T23:36:35.862010Z"
    }
   },
   "outputs": [],
   "source": [
    "ctx_stream.start()\n",
    "ctx_stream.awaitTermination(10 * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6a7359-8f29-46d4-bd88-6ee77005bbda",
   "metadata": {},
   "source": [
    "_Импорты библиотек._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aec0f85c-7e38-44b7-806d-5dc3e1cf080a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604c685-a2dd-4d17-99a7-8a424e92c5cd",
   "metadata": {},
   "source": [
    "# 4. Результаты\n",
    "Вычитаем полученную статистику по словам и визуализируем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a15d573-0c67-4e13-a07a-faebcd3a446a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNABLE_TO_INFER_SCHEMA] Unable to infer schema for JSON. It must be specified manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mjson(hdfs \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/reddit_titles.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m pandas_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[0;32m      5\u001b[0m sorted_df \u001b[38;5;241m=\u001b[39m pandas_df\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\sparked\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:425\u001b[0m, in \u001b[0;36mDataFrameReader.json\u001b[1;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mjson(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator: Iterable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\sparked\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\sparked\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for JSON. It must be specified manually."
     ]
    }
   ],
   "source": [
    "df = spark.read.json(hdfs + \"/reddit_titles.json\")\n",
    "\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "sorted_df = pandas_df.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(sorted_df[\"word\"][:10], sorted_df[\"count\"][:10])  # Топ-10 слов\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Top 10 Words\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
