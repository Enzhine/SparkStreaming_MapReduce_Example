{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51a92df6ec7b746f",
   "metadata": {},
   "source": [
    "# Распределенные вычисления ДЗ-2 | Шамаев Онар Евгеньевич "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7db5f2c875c41a",
   "metadata": {},
   "source": [
    "## Постановка задачи\n",
    "\n",
    "Необходимо в потоковом формате считывать последнюю активность сообщества r/AskReddit платформы Reddit.\n",
    "Целью будет найти самые встречающиеся слова.\n",
    "Необходимо использовать Spark Streaming сохраняя данные на HDFS.\n",
    "\n",
    "## План\n",
    "1. Создание TCP сервера (RSS читателя заголовков)\n",
    "2. Запуск HDFS\n",
    "3. Написать MapReduce для Spark Streaming\n",
    "4. Кеширование\n",
    "5. Результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed36cf5044f39634",
   "metadata": {},
   "source": [
    "## 1. Создание TCP сервера (RSS читателя заголовков)\n",
    "\n",
    "Идея технологии RSS заключается в том, чтобы предоставлять последние N текстовых информативных блоков чего-либо. Например на новостных сайтах - это краткая сводка о последних события. В случае Reddit - это список последних назвний последних тем, поднимаемых пользователями в сообществе r/AskReddit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59edecd3c69378db",
   "metadata": {},
   "source": [
    "_Импорты библиотек._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c187257be5d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import socket\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44f67fa79f7eb7",
   "metadata": {},
   "source": [
    "Последние новости (в RSS формате) доступны по ссылке:\n",
    "`https://www.reddit.com/r/AskReddit/new/.rss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3c96c2e9463f890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:35:29.320248Z",
     "start_time": "2025-03-07T23:35:29.316579Z"
    }
   },
   "outputs": [],
   "source": [
    "rss_link = 'https://www.reddit.com/r/AskReddit/new/.rss'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb3a1a7b5567a7",
   "metadata": {},
   "source": [
    "Определим программу читателя новостной ленты RSS и извлекающей оттуда заголовки, создающий TCP сокет и отправляющий их туда."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:35:31.290957Z",
     "start_time": "2025-03-07T23:35:31.284047Z"
    }
   },
   "outputs": [],
   "source": [
    "encoding = 'utf-8'\n",
    "\n",
    "\n",
    "def rss2tcp_reader(ip, port, interval_sec: float = 30.0, ttl=None):\n",
    "    last_stamp: time.struct_time | None = None\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.bind((ip, port))\n",
    "    s.listen(1)\n",
    "\n",
    "    print(f'Awaiting connection at {ip}:{port}...')\n",
    "    conn, addr = s.accept()\n",
    "    print('Connected by ', addr)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            if (ttl is not None) and last_stamp > ttl:\n",
    "                print('Time to live passed. Exiting.')\n",
    "                break\n",
    "\n",
    "            feed = feedparser.parse(rss_link)\n",
    "            if feed.status != 200:\n",
    "                print(f'Bad response {feed.status} received! Exiting.')\n",
    "                break\n",
    "\n",
    "            _mx_stmp = None\n",
    "            for i, entry in enumerate(feed.entries):\n",
    "                title = entry.title\n",
    "                time_stamp = entry.published_parsed\n",
    "\n",
    "                if (last_stamp is None) or (last_stamp < time_stamp):\n",
    "                    if _mx_stmp is None: _mx_stmp = time_stamp\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "                conn.send(title.encode(encoding) + b'\\n')\n",
    "            if _mx_stmp is not None:\n",
    "                last_stamp = _mx_stmp\n",
    "\n",
    "            time.sleep(interval_sec)\n",
    "    except Exception as ex:\n",
    "        print(f'Exception happened {ex}')\n",
    "    finally:\n",
    "        s.close()\n",
    "        print(f'Closed connection')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977904fc9140cece",
   "metadata": {},
   "source": [
    "Данная программа представлена отдельно в файле `tcp_server.py`. Запустим ее независмо, чтобы можно было запускать ячейки ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd598312c1dbb54",
   "metadata": {},
   "source": [
    "## 2. Запуск HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1b92c7bdd23410",
   "metadata": {},
   "source": [
    "Воспользуемся наработками 1 дз. Поднимем HDFS кластер из 1 namenode и 2 datanode с помощью Docker.\n",
    "\n",
    "Запустим кластер из папки `compose` командой `docker-compose -f \"docker.compose.yml\" up -d`.\n",
    "\n",
    "UI кластера доступен по адресу `http://localhost:9870/`.\n",
    "\n",
    "Сама HDFS доступна по адресу `hdfs://localhost:8020`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c895aa2a86aec70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:35:37.881064Z",
     "start_time": "2025-03-07T23:35:37.878176Z"
    }
   },
   "outputs": [],
   "source": [
    "hdfs = 'hdfs://localhost:8020'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41eee99e9b48e03",
   "metadata": {},
   "source": [
    "## 3. Написать MapReduce для Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e242cd821b967efc",
   "metadata": {},
   "source": [
    "_Импорты библиотек._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a432bd927b9c64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:35:44.242264Z",
     "start_time": "2025-03-07T23:35:43.422350Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd59f7a4c2105a4",
   "metadata": {},
   "source": [
    "Создадим Спарк-контекст и Стриминговый контекст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "481b085bab7138fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:36:04.236625Z",
     "start_time": "2025-03-07T23:35:46.463067Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ctx = SparkContext(master=\"local[2]\", appName=\"AskRedditWordCounter\")\n",
    "ctx._jsc.hadoopConfiguration().set(\"dfs.client.use.datanode.hostname\", \"true\")\n",
    "spark = SparkSession(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11c642ae033e0d0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:36:06.833515Z",
     "start_time": "2025-03-07T23:36:06.785452Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enzhine\\miniconda3\\envs\\sparked\\Lib\\site-packages\\pyspark\\streaming\\context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batchDuration = 30\n",
    "ctx_stream = StreamingContext(ctx, batchDuration)\n",
    "ctx_stream.checkpoint(hdfs+'/rdd_cp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec2e864c9d2a548",
   "metadata": {},
   "source": [
    "PySpark UI доступен по адресу `http://localhost:4040`.\n",
    "\n",
    "Будем использовать DStream, считывающий данные по TCP соединению, созданном нашим RSS читателем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a146b0e741d29b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:36:15.117157Z",
     "start_time": "2025-03-07T23:36:15.074467Z"
    }
   },
   "outputs": [],
   "source": [
    "ip = 'localhost'\n",
    "port = 9999\n",
    "\n",
    "d_stream = ctx_stream.socketTextStream(ip, port)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82affc2efc435c8d",
   "metadata": {},
   "source": [
    "Определим фильтр, для подготовки к разбиению заголовков на слова, и непосредственно разделитель на слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64c5adb4312582ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:36:17.722114Z",
     "start_time": "2025-03-07T23:36:17.717951Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def prepare(line: str) -> str:\n",
    "    return line.translate({key: ' ' for key in string.punctuation + '?!()$@/'}).lower()\n",
    "\n",
    "\n",
    "def word_splitter(line: str) -> list[str]:\n",
    "    return line.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2080453a720bc6",
   "metadata": {},
   "source": [
    "Определим обработчик RDD по топ, получаемых DStream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "680b962f6a7ef3ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:36:19.682874Z",
     "start_time": "2025-03-07T23:36:19.679227Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import RDD\n",
    "\n",
    "def rdd_handler(time_, rdd: RDD):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "\n",
    "    print(f'RDD handled at {time_}')\n",
    "    df = rdd.toDF(schema=[\"word\", \"count\"])\n",
    "    df.write.format('json').mode('overwrite').save(hdfs + '/askReddit.json')\n",
    "\n",
    "def rdds_summator(new_values, old_state):\n",
    "    print(f'RDD state updated')\n",
    "    return sum(new_values) + (old_state or 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8dedca107976b1",
   "metadata": {},
   "source": [
    "Определим порядок действий джобы Spark Stream:\n",
    "- разделяем входные заголовки на слова;\n",
    "- map в пару (слово, количество), где количество всегда равно 1;\n",
    "- обновление состояния по ключу (reduce) путем сложения количеств;\n",
    "- запись в кластер hdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abea17d87b23bbbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:36:23.423127Z",
     "start_time": "2025-03-07T23:36:23.386365Z"
    }
   },
   "outputs": [],
   "source": [
    "word_stream = d_stream.flatMap(lambda line: word_splitter(prepare(line))) \\\n",
    "                    .map(lambda word: (word, 1)) \\\n",
    "                    .reduceByKey(lambda x, y: x + y) \\\n",
    "                    .updateStateByKey(rdds_summator) \\\n",
    "                    .foreachRDD(rdd_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be61264-51b9-4255-89c1-cca98b0caaa6",
   "metadata": {},
   "source": [
    "Запустим джобу. Теперь она будет работать на фоне и собирать статистику встречаемых слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603969914b3e94e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:38:35.951154Z",
     "start_time": "2025-03-07T23:36:35.862010Z"
    }
   },
   "outputs": [],
   "source": [
    "ctx_stream.start()\n",
    "duration = 5 * 60\n",
    "ctx_stream.awaitTermination(duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6a7359-8f29-46d4-bd88-6ee77005bbda",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a15d573-0c67-4e13-a07a-faebcd3a446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(hdfs + \"/askReddit.json\")\n",
    "\n",
    "# Преобразование Spark DataFrame в Pandas DataFrame\n",
    "pandas_df = df.toDF()\n",
    "\n",
    "# Сортировка данных по количеству вхождений\n",
    "pandas_df = pandas_df.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "# Построение графика\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(pandas_df[\"word\"][:10], pandas_df[\"count\"][:10])  # Топ-10 слов\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Top 10 Words\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
