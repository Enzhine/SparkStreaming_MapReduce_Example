{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Распределенные вычисления ДЗ-2 | Шамаев Онар Евгеньевич ",
   "id": "51a92df6ec7b746f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Постановка задачи\n",
    "\n",
    "Необходимо в потоковом формате считывать последнюю активность сообщества r/AskReddit платформы Reddit.\n",
    "Целью будет найти самые встречающиеся слова.\n",
    "Необходимо использовать Spark Streaming сохраняя данные на HDFS.\n",
    "\n",
    "## План\n",
    "1. Создание TCP сервера (RSS читателя заголовков)\n",
    "2. Запуск HDFS\n",
    "3. Написать MapReduce для Spark Streaming\n",
    "4. Кеширование\n",
    "5. Результаты"
   ],
   "id": "7f7db5f2c875c41a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Создание TCP сервера (RSS читателя заголовков)",
   "id": "ed36cf5044f39634"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "_Импорты библиотек._",
   "id": "59edecd3c69378db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import feedparser\n",
    "import socket\n",
    "import time"
   ],
   "id": "70c187257be5d4cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Последние новости (в RSS формате) доступны по ссылке:\n",
    "`https://www.reddit.com/r/AskReddit/new/.rss`"
   ],
   "id": "ee44f67fa79f7eb7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:35:29.320248Z",
     "start_time": "2025-03-07T23:35:29.316579Z"
    }
   },
   "cell_type": "code",
   "source": "rss_link = 'https://www.reddit.com/r/AskReddit/new/.rss'",
   "id": "f3c96c2e9463f890",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Определим программу читателя новостной ленты RSS и извлекающей оттуда заголовки, создающий TCP сокет и отправляющий их туда.",
   "id": "9ebb3a1a7b5567a7"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-07T23:35:31.290957Z",
     "start_time": "2025-03-07T23:35:31.284047Z"
    }
   },
   "source": [
    "encoding = 'utf-8'\n",
    "\n",
    "\n",
    "def rss2tcp_reader(ip, port, interval_sec: float = 30.0, ttl=None):\n",
    "    last_stamp: time.struct_time | None = None\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.bind((ip, port))\n",
    "    s.listen(1)\n",
    "\n",
    "    print(f'Awaiting connection at {ip}:{port}...')\n",
    "    conn, addr = s.accept()\n",
    "    print('Connected by ', addr)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            if (ttl is not None) and last_stamp > ttl:\n",
    "                print('Time to live passed. Exiting.')\n",
    "                break\n",
    "\n",
    "            feed = feedparser.parse(rss_link)\n",
    "            if feed.status != 200:\n",
    "                print(f'Bad response {feed.status} received! Exiting.')\n",
    "                break\n",
    "\n",
    "            _mx_stmp = None\n",
    "            for i, entry in enumerate(feed.entries):\n",
    "                title = entry.title\n",
    "                time_stamp = entry.published_parsed\n",
    "\n",
    "                if (last_stamp is None) or (last_stamp < time_stamp):\n",
    "                    if _mx_stmp is None: _mx_stmp = time_stamp\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "                conn.send(title.encode(encoding) + b'\\n')\n",
    "            if _mx_stmp is not None:\n",
    "                last_stamp = _mx_stmp\n",
    "\n",
    "            time.sleep(interval_sec)\n",
    "    except Exception as ex:\n",
    "        print(f'Exception happened {ex}')\n",
    "    finally:\n",
    "        s.close()\n",
    "        print(f'Closed connection')"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Запустим программу в отдельном потоке. Сохраним поток в переменную `tcp_server`.",
   "id": "977904fc9140cece"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:35:34.199523Z",
     "start_time": "2025-03-07T23:35:34.196317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ip = 'localhost'\n",
    "port = 9999\n",
    "# tcp_server = multiprocessing.Process(target=rss2tcp_reader, args=(ip, port))\n",
    "# tcp_server.start()"
   ],
   "id": "44537e448a126220",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Запуск HDFS",
   "id": "6dd598312c1dbb54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Воспользуемся наработками 1 дз. Поднимем HDFS кластер из 1 namenode и 2 datanode.\n",
    "\n",
    "UI кластера доступен по адресу `http://localhost:9870/`.\n",
    "\n",
    "Сама система доступна по адресу `hdfs://localhost:8020`."
   ],
   "id": "8f1b92c7bdd23410"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:35:37.881064Z",
     "start_time": "2025-03-07T23:35:37.878176Z"
    }
   },
   "cell_type": "code",
   "source": "hdfs = 'hdfs://localhost:8020'",
   "id": "5c895aa2a86aec70",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Написать MapReduce для Spark Streaming",
   "id": "f41eee99e9b48e03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "_Импорты библиотек._",
   "id": "e242cd821b967efc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:35:44.242264Z",
     "start_time": "2025-03-07T23:35:43.422350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ],
   "id": "c8a432bd927b9c64",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Создадим Спарк-контекст и Стриминговый контекст.",
   "id": "acd59f7a4c2105a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:36:04.236625Z",
     "start_time": "2025-03-07T23:35:46.463067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ctx = SparkContext(master=\"local[1]\", appName=\"AskRedditWordCounter\")\n",
    "spark = SparkSession(ctx)"
   ],
   "id": "481b085bab7138fa",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:36:06.833515Z",
     "start_time": "2025-03-07T23:36:06.785452Z"
    }
   },
   "cell_type": "code",
   "source": "ctx_stream = StreamingContext(ctx, 1)",
   "id": "11c642ae033e0d0e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onaru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\streaming\\context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "PySpark UI доступен по адресу `http://localhost:4040`.\n",
    "\n",
    "Будем использовать DStream, считывающий данные по TCP соединению нашего RSS читателя."
   ],
   "id": "dec2e864c9d2a548"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:36:15.117157Z",
     "start_time": "2025-03-07T23:36:15.074467Z"
    }
   },
   "cell_type": "code",
   "source": "d_stream = ctx_stream.socketTextStream(ip, port)",
   "id": "8a146b0e741d29b4",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Определим фильтр, для подготовки к разбиению заголовков на слова, и непосредственно разделитель на слова.",
   "id": "82affc2efc435c8d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:36:17.722114Z",
     "start_time": "2025-03-07T23:36:17.717951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def prepare(line: str) -> str:\n",
    "    return line.translate({key: ' ' for key in string.punctuation})\n",
    "\n",
    "\n",
    "def word_splitter(line: str) -> list[str]:\n",
    "    return line.split(' ')"
   ],
   "id": "64c5adb4312582ca",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Определим обработчик RDD, получаемых",
   "id": "6f2080453a720bc6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:36:19.682874Z",
     "start_time": "2025-03-07T23:36:19.679227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark import RDD\n",
    "\n",
    "\n",
    "def rdd_handler(time_, rdd: RDD):\n",
    "    print(f'RDD handled at {time_}')\n",
    "    df = rdd.toDF(schema=[\"word\", \"count\"])\n",
    "    df.write.format('parquet').mode('overwrite').save(hdfs + '/askReddit.parquet')"
   ],
   "id": "680b962f6a7ef3ac",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Определим порядок действий джобы Spark Stream.",
   "id": "5b8dedca107976b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:36:23.423127Z",
     "start_time": "2025-03-07T23:36:23.386365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_stream = d_stream.flatMap(lambda line: word_splitter(prepare(line.decode(encoding))))\n",
    "\n",
    "words_mapped = word_stream.map(lambda word: (word, 1))\n",
    "\n",
    "words_counts = words_mapped.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "words_counts.foreachRDD(rdd_handler)"
   ],
   "id": "abea17d87b23bbbc",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:38:35.951154Z",
     "start_time": "2025-03-07T23:36:35.862010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ctx_stream.start()\n",
    "ctx_stream.awaitTermination(120)"
   ],
   "id": "603969914b3e94e1",
   "outputs": [],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
